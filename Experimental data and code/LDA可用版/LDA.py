# -*- coding:utf-8 -*-
# Author:Zhou Yang




# file_index = [(34, 245), (34, 51), (34, 252), (34, 106), (34, 36), (34, 0), (34, 161), (34, 3), (34, 48), (34, 4), (70, 169), (70, 316), (70, 180), (70, 291), (70, 304), (70, 38), (70, 222), (76, 175), (76, 211), (76, 27), (76, 266), (76, 307), (76, 203), (87, 207), (87, 185), (87, 307), (87, 31), (87, 119), (87, 77), (87, 315), (87, 167), (87, 66), (87, 131), (87, 301), (87, 47), (92, 35), (92, 285), (92, 111), (92, 247), (92, 184), (92, 279), (92, 158), (92, 248), (92, 47), (92, 268), (92, 19), (102, 28), (102, 203), (102, 181), (102, 190), (102, 139), (102, 323), (102, 1), (102, 105), (102, 149), (102, 66), (102, 55), (110, 41), (110, 256), (110, 3), (110, 124), (110, 296), (110, 81), (110, 59), (110, 288), (110, 276), (110, 114), (110, 171), (110, 52), (110, 19), (110, 63), (110, 39), (110, 73), (110, 242), (110, 211), (110, 139), (110, 123), (110, 264), (125, 172), (125, 87), (125, 147), (125, 17), (125, 16), (125, 82), (125, 152), (125, 101), (125, 43), (125, 109), (139, 1), (139, 87), (139, 263), (139, 165), (139, 314), (139, 269), (139, 3), (139, 47), (139, 85), (139, 103), (139, 75), (139, 194), (139, 280), (139, 46), (139, 29), (139, 74), (139, 309), (139, 215), (139, 112), (139, 154), (139, 177), (139, 176), (148, 126), (148, 237), (148, 58), (148, 59), (148, 143), (148, 260), (148, 278), (148, 223), (148, 196), (148, 116), (188, 34), (188, 318), (188, 231), (188, 64), (188, 187), (188, 192), (188, 22), (188, 93), (188, 121), (203, 89), (203, 92), (203, 319), (203, 142), (203, 56), (203, 153), (203, 242), (203, 40), (203, 249), (203, 287), (203, 157), (206, 234), (206, 74), (206, 2), (206, 191), (211, 174), (211, 317), (211, 275), (211, 225), (211, 57), (211, 229), (211, 135), (211, 45), (211, 241), (211, 3), (211, 13), (211, 305), (211, 138), (211, 205), (211, 210), (211, 5), (211, 77), (211, 282), (211, 27), (211, 160), (211, 87), (211, 214), (211, 285), (211, 292), (211, 206), (211, 40), (211, 50), (211, 136), (211, 260), (211, 42), (211, 70), (211, 86), (211, 108), (211, 202), (211, 148), (211, 220), (211, 34), (211, 203), (211, 254), (211, 250), (211, 279), (211, 166), (211, 30), (211, 188), (211, 32), (211, 233), (211, 129), (211, 280), (211, 321), (229, 25), (229, 18), (229, 312), (229, 265), (229, 6), (229, 212), (229, 224), (229, 297), (229, 164), (229, 271), (229, 49), (229, 267), (229, 322), (229, 151), (229, 160), (229, 21), (229, 130), (229, 115), (249, 213), (249, 24), (249, 274), (249, 173), (249, 239), (249, 236), (249, 98), (249, 199), (249, 294), (249, 186), (249, 53), (249, 23), (249, 251), (279, 128), (279, 289), (279, 104), (279, 125), (279, 314), (279, 1), (279, 306), (279, 188), (279, 92), (279, 144), (279, 83), (279, 88), (279, 227), (279, 259), (279, 65), (279, 219), (279, 310), (279, 7), (280, 80), (280, 290), (280, 148), (280, 142), (280, 182), (280, 132), (314, 134), (314, 34), (314, 91), (314, 253), (314, 299), (314, 228), (314, 308), (314, 258), (314, 195), (314, 281), (314, 243)]
#
# file_set = [0, 1, 2, 3, 4, 5, 6, 7, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 34, 35, 36, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 63, 64, 65, 66, 70, 73, 74, 75, 76, 77, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 98, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 114, 115, 116, 119, 121, 123, 124, 125, 126, 128, 129, 130, 131, 132, 134, 135, 136, 138, 139, 142, 143, 144, 147, 148, 149, 151, 152, 153, 154, 157, 158, 160, 161, 164, 165, 166, 167, 169, 171, 172, 173, 174, 175, 176, 177, 180, 181, 182, 184, 185, 186, 187, 188, 190, 191, 192, 194, 195, 196, 199, 202, 203, 205, 206, 207, 210, 211, 212, 213, 214, 215, 219, 220, 222, 223, 224, 225, 227, 228, 229, 231, 233, 234, 236, 237, 239, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 263, 264, 265, 266, 267, 268, 269, 271, 274, 275, 276, 278, 279, 280, 281, 282, 285, 287, 288, 289, 290, 291, 292, 294, 296, 297, 299, 301, 304, 305, 306, 307, 308, 309, 310, 312, 314, 315, 316, 317, 318, 319, 321, 322, 323]
#
# file_index_new = list()
#
#
# for i in range(len(file_index)):
#      A, B = file_index[i][0], file_index[i][1]
#      file_index_new.append((file_set.index(A), file_set.index(B)))
#
# print(file_index_new)
from sklearn.feature_extraction.text import CountVectorizer

if __name__ == "__main__":

     # 存储读取语料 一行语料为一个文档
     corpus = []
     numm = 0
     for line in open('D5.txt', 'r', encoding="utf8").readlines():
          # print line
          # if numm in file_set:
          #     corpus.append(line.strip())
          # numm += 1
          # print(corpus)
          corpus.append(line.strip())
     # 5，10，20，50
     topic_num = 20




     # 将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频
     vectorizer = CountVectorizer()
     print(vectorizer)

     X = vectorizer.fit_transform(corpus)
     analyze = vectorizer.build_analyzer()
     weight = X.toarray()

     print(len(weight))
     print(weight[:35, :35])

     # LDA算法
     print('LDA:')
     import numpy as np
     import lda
     #import lda.datasets

     np.set_printoptions(suppress=True)

     model = lda.LDA(n_topics=topic_num, n_iter=500, random_state=1)
     model.fit(np.asarray(weight))  # model.fit_transform(X) is also available
     topic_word = model.topic_word_  # model.components_ also works

     print("------")
     print(model)
     print("------")

     # print(topic_word.shape)

     word = vectorizer.get_feature_names()

     label = []
     # 主题-词语（Topic-Word）分布
     for n in range(topic_num):
          # topic_most_pr = topic_word[n]
          topic_most_pr = topic_word[n].argmax()
          label.append(topic_most_pr)
          print("topic: {} word: {}".format(n, word[topic_most_pr]))



     # 文档-主题（Document-Topic）分布
     doc_topic = model.doc_topic_
     print("type(doc_topic): {}".format(type(doc_topic)))
     print("shape: {}".format(doc_topic.shape))

     # print(doc_topic)

     # 选择对应词的大小值
     for i in doc_topic:
          # print("[" + str(i[2]) + "," + str(i[4]) + "," + str(i[5]) + "," + str(i[7]) + "," + str(i[9]) + "],")
          # print("[" + str(i[0]) + "," + str(i[1]) + "," + str(i[2]) + "," + str(i[3]) + "," + str(i[4]) + "],")
          # print(list(i), end=",")
          print(list(i))

     # # 输出前10篇文章最可能的Topic
     # label = []
     # for n in range(topic_num):
     #      topic_most_pr = doc_topic[n]
     #      topic_most_pr = doc_topic[n].argmax()
     #      label.append(topic_most_pr)
     #      print("doc: {} topic: {}".format(n, word[topic_most_pr]))


     # print(word)





     #      # 计算文档主题分布图
     # import matplotlib.pyplot as plt
     #
     # f, ax = plt.subplots(6, 1, figsize=(8, 8), sharex=True)
     # for i, k in enumerate([0, 1, 2, 3, 8, 9]):
     #      ax[i].stem(doc_topic[k, :], linefmt='r-',
     #                 markerfmt='ro', basefmt='w-')
     #      ax[i].set_xlim(-1, 2)  # x坐标下标
     #      ax[i].set_ylim(0, 1.2)  # y坐标下标
     #      ax[i].set_ylabel("Prob")
     #      ax[i].set_title("Document {}".format(k))
     # ax[5].set_xlabel("Topic")
     # plt.tight_layout()
     # plt.show()



